{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e137daac-a72e-449a-b14d-f93d21fd7052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import List\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "from IPython.display import Image, display\n",
    "from pprint import pprint\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c560126-ce92-4c17-802f-e37b97e3db30",
   "metadata": {},
   "source": [
    "**Set Up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ba79e9-62ea-467f-9752-fd4d629cbe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PINECONE_API_KEY'] = 'xxx'\n",
    "os.environ['OPENAI_API_KEY'] = 'xxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d992af1-cca4-4e1c-9614-0cd03ade7511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pinecone setup\n",
    "index_name = 'news-embedding-stitching'\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5f7d545-f4d2-4a9b-84e1-63e1f0076e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 2048, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=4, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=4, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=4, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=4, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=4, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=4, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=4, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=4, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Finetuned model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "login(token='xxx')\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"Jiangying9/SmolLM2-1.7B-Instruct-FineTuned6\")\n",
    "tokenizer2.add_eos_token = True\n",
    "tokenizer2.pad_token_id = 0\n",
    "tokenizer2.padding_side = \"left\"\n",
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\"Jiangying9/SmolLM2-1.7B-Instruct-FineTuned6\")\n",
    "fine_tuned_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3fe464-d720-45ac-a9a9-bcb420828b02",
   "metadata": {},
   "source": [
    "**Create Agents**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa58806-21b6-413f-a1a2-a05f0c4ac357",
   "metadata": {},
   "source": [
    "Agent 1: Is the document relevant to the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ae0ac84-5fc7-40a6-b438-03f8a72263a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data model for grading\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# Initialize LLM and structured output grader\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments, method=\"function_calling\")\n",
    "\n",
    "# Define the grading system prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "# Set up the prompt template\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44a5e4bf-8bb8-47b8-a07b-a43eab00e10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_grade(question: str):\n",
    "    # Retrieve relevant documents using the retriever\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    # Iterate over retrieved documents and grade each one\n",
    "    grades = []\n",
    "    for doc in retrieved_docs:\n",
    "        document_text = doc.page_content  \n",
    "        \n",
    "        print(\"Retrieved Document:\\n\", document_text)\n",
    "        \n",
    "        # Get the grade for the document\n",
    "        grade_response = structured_llm_grader.invoke(grade_prompt.format_messages(document=document_text, question=question))\n",
    "        \n",
    "        # Access the binary_score directly from the GradeDocuments object\n",
    "        grade = grade_response.binary_score\n",
    "        grades.append(grade)\n",
    "        \n",
    "        print(f\"Grade: {grade}\\n\")\n",
    "    \n",
    "    return grades\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f20c1ed6-a6f7-426d-bd8e-d0f015f41c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fy/9k597j093cd6hcn99bcl7w500000gn/T/ipykernel_35982/4197342762.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Document:\n",
      " https://www.huffpost.com/entry/vulnerable-house-dems-see-abortion-as-winning-campaign-theme_n_62e6847ee4b006483a9e59ea Vulnerable House Dems See Abortion As Winning Campaign Theme POLITICS The Supreme Court decision on abortion has scrambled the political dynamics heading into the November elections, when control of Congress is at stake. Thomas Beaumont, AP 2022-07-31\n",
      "Grade: yes\n",
      "\n",
      "Retrieved Document:\n",
      " https://www.huffpost.com/entry/election-2022-midterms-ohio-indiana_n_6270f5b2e4b029505df61205 2022 Midterms: What To Know About Tuesday's Primaries In Ohio And Indiana POLITICS The races, particularly in Ohio, could provide a fresh window into former President Trump's sway among the party faithful. Julie Carr Smyth and Tom Davies, AP 2022-05-03\n",
      "Grade: no\n",
      "\n",
      "Retrieved Document:\n",
      " https://www.huffpost.com/entry/tim-scott-senate-republicans-november-election_n_631ef3a4e4b027aa405d4f5e Sen. Tim Scott Downplays Electability Concerns Over Struggling Senate GOP Candidates POLITICS \"Who we have on the field is who we’re gonna play,” Scott told \"Fox News Sunday.\" Marita Vlachou 2022-09-12\n",
      "Grade: no\n",
      "\n",
      "Retrieved Document:\n",
      " https://www.huffpost.com/entry/timothy-snyder-donald-trump-2024_n_6173c0d6e4b010d933112eab Tyranny Expert Sounds The Alarm On 2024 Election: It’s Happening POLITICS Yale history professor Timothy Snyder pointed out the worrying ways the next election could differ from 2020. Lee Moran 2021-10-23\n",
      "Grade: no\n",
      "\n",
      "['yes', 'no', 'no', 'no']\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the main policy issues for the upcoming election?\"\n",
    "graded_docs = retrieve_and_grade(question)\n",
    "print(graded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefcc8ae-bd41-4310-8013-aac3ff0ecc5f",
   "metadata": {},
   "source": [
    "**Basic Rag**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd71b052-c0bf-40ce-9624-cba9651602bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_prompt = PromptTemplate.from_template(\"Using the following context, answer the question: \\nContext: {context}\\nQuestion: {query}\")\n",
    "basic_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "basic_rag_chain = (\n",
    "    {\"context\": basic_retriever, \"query\": RunnablePassthrough()} \n",
    "    | basic_prompt \n",
    "    | llm.with_config({\"temperature\": 0.7})\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "230c6284-18c4-420c-84f1-3fd454cf06a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation= basic_rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a724c2f-a741-4b58-bdcc-839f4e3c6b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The main policy issues for the upcoming election include abortion, the Supreme Court decision on abortion, control of Congress, former President Trump's sway among party faithful, and electability concerns over struggling Senate GOP candidates.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9171244e-a14b-4952-a634-32697d0717d2",
   "metadata": {},
   "source": [
    "Agent 2: Did RAG Hallucinate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "988d193a-62ec-44a0-ab63-74d821af90e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations, method=\"function_calling\")\n",
    "\n",
    "# Prompt for hallucination grading\n",
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
    "    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combine the prompt with the hallucination grader\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67ecc5cf-da85-4699-96a6-536a1de2afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_generation_with_hallucination(question: str):\n",
    "    #Retrieve the relevant context using the RAG pipeline\n",
    "    retrieved_docs = basic_retriever.get_relevant_documents(question)\n",
    "    context = [doc.page_content for doc in retrieved_docs]  # List of documents' text\n",
    "    \n",
    "    #Generate the answer from the RAG chain\n",
    "    generation = basic_rag_chain.invoke(question)\n",
    "    \n",
    "    #Use the hallucination grader to check if the answer is grounded in the context\n",
    "    grade_response = hallucination_grader.invoke({\"documents\": context, \"generation\": generation})\n",
    "    \n",
    "    #Return the hallucination grade\n",
    "    return grade_response.binary_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bcafb787-9e46-4ce5-8dbf-6a070cf79c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the main policy issues for the upcoming election?\"\n",
    "hallucination_score = evaluate_rag_generation_with_hallucination(question)\n",
    "print(hallucination_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f7feb3-8e2b-49c3-8be2-6cecd201a29f",
   "metadata": {},
   "source": [
    "Agent 3: Did the RAG answer the Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2decf387-880e-4d08-a247-4456070d9077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model for grading answers\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess if the answer addresses the question.\"\"\"\n",
    "    \n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer, method=\"function_calling\")\n",
    "\n",
    "# Prompt template to evaluate if the answer addresses the question\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves the question. \\n\n",
    "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer resolves the question.\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Grader that uses the structured LLM and the answer prompt\n",
    "answer_grader = answer_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "424559a9-df4f-44b9-a1c5-efbf0f4b6a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate a RAG generation using the answer grader\n",
    "def evaluate_rag_generation_with_answer(question: str):\n",
    "    #Retrieve the relevant context using the RAG pipeline\n",
    "    retrieved_docs = basic_retriever.get_relevant_documents(question)\n",
    "    context = [doc.page_content for doc in retrieved_docs]\n",
    "    \n",
    "    #Generate the answer from the RAG chain\n",
    "    generation = basic_rag_chain.invoke(question)\n",
    "    \n",
    "    #Use the answer grader to check if the answer resolves the question\n",
    "    grade_response = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "    \n",
    "    #Return the answer grade\n",
    "    return grade_response.binary_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6be9d57-0491-4151-b311-2620e34231ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the main policy issues for the upcoming election?\"\n",
    "answer_score = evaluate_rag_generation_with_answer(question)\n",
    "print(answer_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43090d7-d5dd-45ef-a193-0c02141a430c",
   "metadata": {},
   "source": [
    "Agent: Rewrite Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb0fd22a-781b-445b-8453-2fcc726604a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question Re-writer\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "022106c4-7633-4347-b04b-0248c35c72fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rewrite a question for vectorstore retrieval\n",
    "def rewrite_question_for_retrieval(question: str):    \n",
    "    #Use the question rewriter pipeline to optimize the question\n",
    "    improved_question = question_rewriter.invoke({\"question\": question})\n",
    "    \n",
    "    return improved_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "020cc18d-0e4d-4d32-b82a-6bcfe63fd9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the key policies being discussed in the upcoming election?\"\n",
    "improved_question = rewrite_question_for_retrieval(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "651aa575-04c6-401b-b0af-9e9130fa441b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question: What are the key policies being discussed in the upcoming election?\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original Question: {question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d15295c9-f982-4f5e-88b3-afd59619cdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Question: What are the main policy topics up for debate in the upcoming election?\n"
     ]
    }
   ],
   "source": [
    "print(f\"Improved Question: {improved_question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1022874f-7216-4d9c-8024-1d75153c7351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define generator parameters\n",
    "generator_params = {\n",
    "    \"max_new_tokens\": 250, \n",
    "    \"temperature\": 0.7,  \n",
    "    \"top_p\": 0.9,  \n",
    "    \"do_sample\": True, \n",
    "    \"pad_token_id\": tokenizer2.eos_token_id,  \n",
    "    \"eos_token_id\": tokenizer2.eos_token_id \n",
    "}\n",
    "\n",
    "def process_trump_style(input_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Converts the input text into a Trump-like style using the fine-tuned model.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The text to be translated into Trump-like style.\n",
    "\n",
    "    Returns:\n",
    "        str: The refined output after processing in Trump-like style.\n",
    "    \"\"\"\n",
    "    # Define the question template for the model\n",
    "    question_template = '''You will be asked to rewrite the following text in about 50 words in the style and tone of Donald Trump. Place four asterisks (****) before the response. Do not add any additional text after the answer.\n",
    "\n",
    "Text:\n",
    "{description_text}\n",
    "\n",
    "Response:'''\n",
    "\n",
    "    # Extract only the 'generation' part from the input dictionary\n",
    "    input_text = input_dict.get('generation', '')\n",
    "\n",
    "    # Format the input text into the model's prompt structure\n",
    "    prompt = question_template.format(description_text=input_text)\n",
    "\n",
    "    # Encode the formatted prompt\n",
    "    inputs = tokenizer2.encode(prompt, return_tensors=\"pt\").to(fine_tuned_model.device)\n",
    "\n",
    "    # Generate the output from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = fine_tuned_model.generate(inputs, **generator_params)\n",
    "\n",
    "    # Decode and return the generated response\n",
    "    refined_generation = tokenizer2.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Return the final Trump-like styled response\n",
    "    return {\"generation\": refined_generation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a5a88d-da58-428a-bdaf-7f23ac25fc88",
   "metadata": {},
   "source": [
    "**LangGraph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "19773245-af9f-4d04-9ad9-b2316661c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        rewritten_question: optimized question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "        hallucination_score: score indicating whether generation is grounded in facts\n",
    "        answer_grade: score indicating whether the answer addresses the question\n",
    "    \"\"\"\n",
    "    \n",
    "    question: str\n",
    "    rewritten_question: str  \n",
    "    generation: str\n",
    "    documents: List[str]\n",
    "    hallucination_score: str \n",
    "    answer_grade: str  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bc971b92-2650-4c4b-b2ab-e16f82ccddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (GraphState): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        GraphState: Updated state with the new key \"documents\" containing retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {**state, \"documents\": documents}\n",
    "\n",
    "\n",
    "def generate(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation using basic_rag_chain\n",
    "    generation = basic_rag_chain.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each document using the structured_llm_grader\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        document_text = d.page_content  \n",
    "\n",
    "        # Use structured_llm_grader to grade relevance\n",
    "        grade_response = structured_llm_grader.invoke(grade_prompt.format_messages(document=document_text, question=question))\n",
    "        \n",
    "        # Access the binary score from the response\n",
    "        grade = grade_response.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "\n",
    "def transform_query(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (GraphState): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        GraphState: Updated state with a rephrased question\n",
    "    \"\"\"\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {**state, \"question\": better_question}\n",
    "\n",
    "\n",
    "def evaluate_hallucination(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Grade the hallucination score of the generation.\n",
    "\n",
    "    Args:\n",
    "        state (GraphState): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        GraphState: Updated state with the hallucination score\n",
    "    \"\"\"\n",
    "    print(\"---EVALUATE HALLUCINATION---\")\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    # Grade hallucination\n",
    "    hallucination_score = hallucination_grader.invoke({\n",
    "        \"documents\": documents, \n",
    "        \"generation\": generation\n",
    "    })\n",
    "    \n",
    "    return {**state, \"hallucination_score\": hallucination_score.binary_score}\n",
    "\n",
    "\n",
    "def evaluate_answer(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Grade whether the generated answer addresses the question.\n",
    "\n",
    "    Args:\n",
    "        state (GraphState): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        GraphState: Updated state with the answer grade\n",
    "    \"\"\"\n",
    "    print(\"---EVALUATE ANSWER---\")\n",
    "    question = state[\"question\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    # Grade answer\n",
    "    answer_grade = answer_grader.invoke({\n",
    "        \"question\": question, \n",
    "        \"generation\": generation\n",
    "    })\n",
    "    \n",
    "    return {**state, \"answer_grade\": answer_grade.binary_score}\n",
    "    \n",
    "def process_final_answer(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Process the generated answer through the fine-tuned model before finalizing the answer.\n",
    "\n",
    "    Args:\n",
    "        state (GraphState): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        GraphState: Updated state with the refined answer\n",
    "    \"\"\"\n",
    "    print(\"---PROCESSING THROUGH FINE-TUNED TRUMP-STYLE MODEL---\")\n",
    "    generation = state.get(\"generation\")\n",
    "    \n",
    "    # Refine the generation with the fine-tuned model\n",
    "    refined_generation = process_trump_style(generation)\n",
    "    \n",
    "    return {**state, \"final_generation\": refined_generation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "79145b22-611d-4907-95ca-7a7d7cb3af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state: GraphState) -> str:\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer or regenerate a question.\n",
    "\n",
    "    Args:\n",
    "        state (GraphState): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for the next node to call (\"not_relevant\" or \"relevant\")\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # If no relevant documents, we need to regenerate a new query\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\")\n",
    "        return \"not_relevant\"\n",
    "    else:\n",
    "        # If we have relevant documents, proceed to generate the answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"relevant\"\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state: GraphState) -> str:\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers the question.\n",
    "\n",
    "    Args:\n",
    "        state (GraphState): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for the next node to call (\"useful\", \"not useful\", or \"not supported\")\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    # Evaluate hallucination score\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    hallucination_grade = score.binary_score\n",
    "\n",
    "    # Check if the generation is grounded in the documents\n",
    "    if hallucination_grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        \n",
    "        # Evaluate whether the generation addresses the question\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        answer_grade = score.binary_score\n",
    "        \n",
    "        if answer_grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d9a0648-850f-40cb-a9ee-423b5a0e315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the workflow with the GraphState\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes (functions)\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve documents\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents for relevance\n",
    "workflow.add_node(\"generate\", generate)  # generate the answer based on context\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform the query to make it more precise\n",
    "\n",
    "# Build the graph with edges\n",
    "workflow.add_edge(START, \"retrieve\")  # Start with retrieving documents\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")  # After retrieval, grade documents for relevance\n",
    "\n",
    "# Conditional edges for document relevance\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,  # Function that decides if we need to generate or transform query\n",
    "    {\n",
    "        \"not_relevant\": \"transform_query\",  # If documents are not relevant, rephrase query\n",
    "        \"relevant\": \"generate\",  # If documents are relevant, generate an answer\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")  # If query is transformed, go back to retrieval\n",
    "\n",
    "# Define conditional edges for the 'generate' node based on the decision made\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,  # Function to check if generation answers the question\n",
    "    {\n",
    "        \"not supported\": \"generate\",  # If generation is not supported, retry generation\n",
    "        \"not useful\": \"transform_query\",  # If generation doesn't address the question, transform the query\n",
    "        \"useful\": \"process_trump_style\",  # Only if the decision is to generate, proceed to processing\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add the new step in the workflow\n",
    "workflow.add_node(\"process_trump_style\", process_trump_style)\n",
    "\n",
    "# Connect the 'generate' step to 'process_final_answer' (only when generation occurs)\n",
    "workflow.add_edge(\"process_trump_style\", END) \n",
    "\n",
    "# Compile the workflow into a runnable application\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa8d5d-84d0-4e0e-8f27-6f86b159fceb",
   "metadata": {},
   "source": [
    "**CLI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1976ce27-5c06-4f73-a18b-792881413297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Multi-Agent Question Answering System!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your question:  What are the main policy issues for the upcoming election?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing the question: What are the main policy issues for the upcoming election?\n",
      "\n",
      "---RETRIEVE---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GENERATE---\n",
      "---EVALUATE HALLUCINATION---\n",
      "---EVALUATE ANSWER---\n",
      "Final Answer: You will be asked to rewrite the following text in about 50 words in the style and tone of Donald Trump. Place four asterisks (****) before the response. Do not add any additional text after the answer.\n",
      "\n",
      "Text:\n",
      "The main policy issues for the upcoming election include abortion, the Supreme Court decision on abortion, control of Congress, former President Trump's sway among party faithful, and electability concerns over struggling Senate GOP candidates.\n",
      "\n",
      "Response:\n",
      "Folks, let me tell you, nobody knows more about big league politics than I do. And believe me, the issues on this election are huge. We're talking about abortion, the Supreme Court, Congress, and Trump's influence. It's going to be a winner-take-all battle, folks.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would you like to ask another question? (yes/no):  yes\n",
      "Please enter your question:  What happened to Queen Elizabeth II?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing the question: What happened to Queen Elizabeth II?\n",
      "\n",
      "---RETRIEVE---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GENERATE---\n",
      "---EVALUATE HALLUCINATION---\n",
      "---EVALUATE ANSWER---\n",
      "Final Answer: You will be asked to rewrite the following text in about 50 words in the style and tone of Donald Trump. Place four asterisks (****) before the response. Do not add any additional text after the answer.\n",
      "\n",
      "Text:\n",
      "Queen Elizabeth II sprained her back and missed the Remembrance Sunday service.\n",
      "\n",
      "Response:\n",
      "Folks, let me tell you, Queen Elizabeth is a total loser. She can't even make it to a big event like Remembrance Sunday. Sad!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would you like to ask another question? (yes/no):  no\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for using the system. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Command-line interface to run the multi-agent system with multiple questions\n",
    "def main():\n",
    "    print(\"Welcome to the Multi-Agent Question Answering System!\\n\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"Please enter your question: \")\n",
    "        print(f\"\\nProcessing the question: {question}\\n\")\n",
    "\n",
    "        # Initiate the workflow \n",
    "        state = {\"question\": question}\n",
    "        state = retrieve(state)\n",
    "        state = grade_documents(state)\n",
    "        state = generate(state)\n",
    "\n",
    "        # Proceed through the various steps of the workflow\n",
    "        state = evaluate_hallucination(state)\n",
    "        state = evaluate_answer(state)\n",
    "        state = process_trump_style(state)\n",
    "\n",
    "        # Final decision\n",
    "        print(f\"Final Answer: {state['generation']}\\n\")\n",
    "\n",
    "        # Ask the user if they want to ask another question\n",
    "        another_question = input(\"Would you like to ask another question? (yes/no): \").strip().lower()\n",
    "        if another_question != \"yes\":\n",
    "            print(\"Thank you for using the system. Goodbye!\")\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266208c3-30a2-4a94-bdd0-05c20a730f41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
